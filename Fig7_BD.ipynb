{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwl2182/.conda/envs/a40/lib/python3.7/site-packages/ipykernel_launcher.py:24: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['lines.linewidth'] = 0.7\n",
    "plt.rcParams['legend.fontsize'] = 7\n",
    "plt.rcParams['axes.titlesize'] = 7\n",
    "plt.rcParams['axes.labelsize'] = 7\n",
    "plt.rcParams['xtick.labelsize'] = 7\n",
    "plt.rcParams['ytick.labelsize'] = 7\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "plt.rcParams['xtick.major.size'] = 2\n",
    "plt.rcParams['ytick.major.size'] = 2\n",
    "plt.rcParams['xtick.major.width'] = 0.5\n",
    "plt.rcParams['ytick.major.width'] = 0.5\n",
    "plt.rcParams['xtick.major.pad'] = 2\n",
    "plt.rcParams['ytick.major.pad'] = 2\n",
    "plt.rcParams['xtick.minor.size'] = 1.5\n",
    "plt.rcParams['ytick.minor.size'] = 1.5\n",
    "plt.rcParams['xtick.minor.width'] = 0.4\n",
    "plt.rcParams['ytick.minor.width'] = 0.4\n",
    "plt.rcParams['figure.dpi'] = 225\n",
    "plt.rcParams['figure.figsize'] = [7.5/2.54, 5/2.54]\n",
    "plt.rcParams['path.simplify'] = True\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['image.interpolation'] = None\n",
    "plt.rcParams['image.aspect'] = 'auto'\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['patch.edgecolor'] = 'none'\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.formatter.limits'] = (-3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paton_function(rpe):\n",
    "    a, b, c, d = [-3.5, 11.5, 0.9, 1]\n",
    "    return a + (b / (1+c*np.exp(1-d*x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwl2182/.local/lib/python3.7/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/jwl2182/.local/lib/python3.7/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 1\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 2\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 3\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 4\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 5\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 6\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 7\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 8\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 9\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 10\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 11\n",
      "task positive mode linear_efference_onpolicy efference_scale 1.0 trial 12\n"
     ]
    }
   ],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "num_trials = 100\n",
    "timesteps = 4010\n",
    "num_states = 10\n",
    "num_actions = 10\n",
    "default_actions = 0\n",
    "num_neurons = num_actions\n",
    "\n",
    "beta = 10.0\n",
    "beta_expert = 100.0\n",
    "lr = 0.01\n",
    "lr_V = 0.25\n",
    "efference_noise = 0.0 * np.sqrt(num_neurons)#was 0.05\n",
    "explore_noise= 0.0*np.sqrt(num_neurons)\n",
    "state_noise = 0.0\n",
    "action_noise = 0.0\n",
    "reward_types = [\"r-Q\", \"r-V\"]\n",
    "modes = [\"linear_efference_onpolicy\", \"linear_efference_offpolicy\", \n",
    "         \"rectified_efference_onpolicy\",  \"rectified_efference_offpolicy\"]\n",
    "tasks = [\"positive\"]\n",
    "\n",
    "num_reward_types = len(reward_types)\n",
    "num_modes = len(modes)\n",
    "num_tasks = len(tasks)\n",
    "\n",
    "efference_scale = 1.0\n",
    "expert_beta_options = [0.0, 10.0, 90.0, 990.0]\n",
    "\n",
    "record_every = 1\n",
    "\n",
    "r_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps, len(expert_beta_options)])\n",
    "r_post_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps, len(expert_beta_options)])\n",
    "Q_err_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps, len(expert_beta_options)])\n",
    "Q_corr_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps, len(expert_beta_options)])\n",
    "\n",
    "Q_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps, len(expert_beta_options), num_actions+default_actions, num_states])\n",
    "\n",
    "d1_activity_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps//record_every, len(expert_beta_options), num_neurons])\n",
    "d2_activity_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps//record_every, len(expert_beta_options), num_neurons])\n",
    "\n",
    "action_label_results = np.zeros([num_reward_types, num_tasks, num_modes, num_trials, timesteps//record_every, len(expert_beta_options)])\n",
    "\n",
    "\n",
    "for reward_type in reward_types:\n",
    "    rt = reward_types.index(reward_type)\n",
    "    for task in tasks:\n",
    "        task_id = tasks.index(task)\n",
    "        for mode in modes:\n",
    "            m = modes.index(mode)\n",
    "            for beta_expert in expert_beta_options:\n",
    "                e = expert_beta_options.index(beta_expert)\n",
    "                for trial in range(num_trials):\n",
    "\n",
    "                    print(\"task\", task, \"mode\", mode, \"efference_scale\", efference_scale, 'trial', trial)\n",
    "\n",
    "\n",
    "                \n",
    "                    if task == \"positive\":\n",
    "                        Q_gt = np.zeros([num_states, num_actions+default_actions])\n",
    "                        for s in range(num_states):\n",
    "                            Q_gt[s, s%(num_actions+default_actions)] = 1\n",
    "\n",
    "                    elif task == \"negative\":\n",
    "                        Q_gt = np.zeros([num_states, num_actions+default_actions])\n",
    "                        for s in range(num_states):\n",
    "                            Q_gt[s, s%(num_actions+default_actions)] = -1\n",
    "                    elif task == \"mix\":\n",
    "                        Q_gt = np.zeros([num_states, num_actions+default_actions])\n",
    "                        for s in range(num_states):\n",
    "                            Q_gt[s, 0] = ((s % 2)*2-1)*1\n",
    "                    elif task == \"random\":\n",
    "                         Q_gt = np.random.normal(0, 1, size=(num_states, num_actions))\n",
    "\n",
    "                    V = np.zeros([num_states])\n",
    "                    #if default_actions > 0:\n",
    "                    #    Q_gt = np.concatenate([Q_gt, np.zeros([num_states, default_actions])], axis=-1)\n",
    "                    w_d1 = np.ones([num_neurons, num_states]) / np.sqrt(num_neurons)\n",
    "                    w_d2 = np.ones([num_neurons, num_states]) / np.sqrt(num_neurons)\n",
    "\n",
    "\n",
    "\n",
    "                    zeta_d1 = np.random.random(size=(num_actions, num_neurons)) / num_neurons\n",
    "                    zeta_d2 = np.random.random(size=(num_actions, num_neurons)) / num_neurons\n",
    "\n",
    "                    zeta_d1 *= 0\n",
    "                    zeta_d2 *= 0\n",
    "\n",
    "                    for a in range(num_actions):\n",
    "                        chunk = num_neurons//num_actions\n",
    "\n",
    "                        #zeta_d1[a, np.random.choice(num_neurons, size=(chunk,), replace=False)] = 1\n",
    "                        #zeta_d2[a, np.random.choice(num_neurons, size=(chunk,), replace=False)] = 1\n",
    "                        zeta_d1[a, a*chunk:(a+1)*chunk] = 1.0 / chunk\n",
    "                        zeta_d2[a, a*chunk:(a+1)*chunk] = 1.0 / chunk\n",
    "\n",
    "\n",
    "                    if default_actions > 0:\n",
    "                        zeta_d1 = np.concatenate([zeta_d1, -np.sum(zeta_d1, axis=0, keepdims=True)], axis=0)\n",
    "                        zeta_d2 = np.concatenate([zeta_d2, -np.sum(zeta_d2, axis=0, keepdims=True)], axis=0)\n",
    "\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    zeta_d1[0] *= 0\n",
    "                    zeta_d2[0] *= 0\n",
    "                    for a in range(num_actions):\n",
    "                        chunk = int(num_neurons*0.01)\n",
    "\n",
    "                        #zeta_d1[a, np.random.choice(num_neurons, size=(chunk,), replace=False)] = 1\n",
    "                        #zeta_d2[a, np.random.choice(num_neurons, size=(chunk,), replace=False)] = 1\n",
    "                        zeta_d1[a, a*chunk:(a+1)*chunk] = 1.0 / chunk\n",
    "                        zeta_d2[a, a*chunk:(a+1)*chunk] = 1.0 / chunk\n",
    "\n",
    "                    '''\n",
    "                    zeta_d1 = zeta_d1 / np.linalg.norm(zeta_d1, ord=2, axis=-1, keepdims=True)\n",
    "                    zeta_d2 = zeta_d2 / np.linalg.norm(zeta_d2, ord=2, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "                    lr_p = lr# / (1.0+efference_scale)\n",
    "\n",
    "                    r_hist = []\n",
    "                    Q_accuracy_hist = []\n",
    "                    post_efference_d1 = 0\n",
    "                    post_efference_d2 = 0\n",
    "                    for t in range(timesteps):\n",
    "                        state = np.random.choice(num_states)\n",
    "                        state_onehot = np.zeros([num_states])+state_noise*np.random.normal(0, 1, size=(num_states))\n",
    "                        state_onehot[state] = 1.0#*10**((np.random.random()-0.5)*0.1)\n",
    "                        pre_a_d1 = np.matmul(w_d1, state_onehot)+explore_noise*np.random.normal(0, 1, size=(num_neurons))\n",
    "                        pre_a_d2 = np.matmul(w_d2, state_onehot)+explore_noise*np.random.normal(0, 1, size=(num_neurons))\n",
    "                        a_d1 = np.clip(pre_a_d1, 0, np.inf)\n",
    "                        a_d2 = np.clip(pre_a_d2, 0, np.inf)\n",
    "\n",
    "\n",
    "                        #a_d2 = a_d2_mu+explore_noise*np.random.normal(0, 1, size=(num_neurons))\n",
    "                        Q_d1 = np.matmul(zeta_d1, a_d1)\n",
    "                        Q_d2 = np.matmul(zeta_d2, a_d2)\n",
    "                        Q = Q_d1 - Q_d2 + action_noise*np.random.normal(0, 1, size=(num_actions))\n",
    "\n",
    "                        \n",
    "                        log_probs = (beta*Q)\n",
    "                        \n",
    "                        log_probs -= np.max(log_probs)\n",
    "                        p = np.exp(log_probs)/np.sum(np.exp(log_probs))\n",
    "\n",
    "                        if np.isnan(np.max(p)):\n",
    "                            action = np.random.choice(np.flatnonzero(Q == Q.max()))\n",
    "                        else:\n",
    "                            try:\n",
    "                                action = np.random.choice(np.arange(num_actions+default_actions), p=p)\n",
    "                            except:\n",
    "                                action = np.argmax(Q)\n",
    "\n",
    "                        orig_action = action\n",
    "                        log_probs *= 0\n",
    "                        log_probs[orig_action] = beta\n",
    "                        log_probs += Q_gt[state] * beta_expert * (t % 2)\n",
    "                        log_probs -= np.max(log_probs)\n",
    "                        p = np.exp(log_probs)/np.sum(np.exp(log_probs))\n",
    "\n",
    "                        if np.isnan(np.max(p)):\n",
    "                            action = np.random.choice(np.flatnonzero(Q == Q.max()))\n",
    "                        else:\n",
    "                            try:\n",
    "                                action = np.random.choice(np.arange(num_actions+default_actions), p=p)\n",
    "                            except:\n",
    "                                action = np.argmax(Q)\n",
    "                                \n",
    "\n",
    "                        r = Q_gt[state, action]\n",
    "\n",
    "                        #print(Q_d1, Q_d2, Q, p, action, r, Q[action], r-Q[action], V, r-V)\n",
    "                        if t % 2 == 0:\n",
    "                            r_results[rt,task_id, m, trial, t//2, e] = r\n",
    "                            Q_err_results[rt,task_id, m, trial, t//2, e] = np.mean((Q- Q_gt[state])**2)#np.corrcoef(Q, Q_gt[state])[0, 1]\n",
    "                            Q_corr_results[rt,task_id, m, trial, t//2, e] = np.corrcoef(Q, Q_gt[state])[0, 1]\n",
    "\n",
    "                        randfactor = 1#10.0**((np.random.rand()-0.5))\n",
    "\n",
    "                        if mode.endswith(\"raw\"):\n",
    "                            post_efference_d1 = a_d1 * efference_scale\n",
    "                            post_efference_d2 = a_d2 * efference_scale\n",
    "                        elif mode.endswith(\"wta\"):\n",
    "                            if action < num_actions:\n",
    "                                post_efference_d1 = efference_scale*(zeta_d1[action] / np.linalg.norm(zeta_d1[action], ord=2))\n",
    "                                post_efference_d2 = -efference_scale*(zeta_d2[action] / np.linalg.norm(zeta_d2[action], ord=2))\n",
    "                                post_efference_d2 = post_efference_d2 - np.min(post_efference_d2)\n",
    "                                #post_efference_d2 = post_efference_d2 / (num_actions - 1)\n",
    "                            else:\n",
    "                                post_efference_d2 = -efference_scale*(zeta_d2[action] / np.linalg.norm(zeta_d2[action], ord=2))\n",
    "                                post_inference_d1 = np.zeros([num_neurons])\n",
    "\n",
    "                        elif mode.endswith(\"efference_offpolicy\"):\n",
    "                            post_efference_d1 = np.clip(pre_a_d1 + efference_scale*(zeta_d1[action] / np.linalg.norm(zeta_d1[action], ord=2)), 0, np.inf)\n",
    "                            post_efference_d2 = np.clip(pre_a_d2 + efference_scale*(zeta_d2[action] / np.linalg.norm(zeta_d2[action], ord=2)), 0, np.inf)\n",
    "                        \n",
    "                        elif mode.endswith(\"efference_onpolicy\"):\n",
    "                            post_efference_d1 = np.clip(pre_a_d1 + efference_scale*(zeta_d1[orig_action] / np.linalg.norm(zeta_d1[orig_action], ord=2)), 0, np.inf)\n",
    "                            post_efference_d2 = np.clip(pre_a_d2 + efference_scale*(zeta_d2[orig_action] / np.linalg.norm(zeta_d2[orig_action], ord=2)), 0, np.inf)\n",
    "                        \n",
    "                        else:\n",
    "                            assert False\n",
    "\n",
    "                        #print('mode', mode, action, post_efference_d1, post_efference_d2)\n",
    "                        #post_efference_d1 = np.clip(post_efference_d1, 0, np.inf)\n",
    "                        #post_efference_d2 = np.clip(post_efference_d2, 0, np.inf)\n",
    "\n",
    "                        if reward_type == \"r-V\":\n",
    "                            RPE = r - V[state]\n",
    "                        elif reward_type == \"r-Q\":\n",
    "                            RPE = r - Q[action]\n",
    "                        elif reward_type == \"r\":\n",
    "                            RPE = r\n",
    "                        else:\n",
    "                            assert False\n",
    "                        if mode.startswith(\"linear\"):\n",
    "                            DA_term_d1 = RPE\n",
    "                            DA_term_d2 = -RPE\n",
    "\n",
    "                        elif mode.startswith(\"rectified\"):\n",
    "                            DA_term_d1 = max(0, RPE)\n",
    "                            DA_term_d2 = min(0, -RPE)\n",
    "\n",
    "                        else:\n",
    "                            assert False\n",
    "\n",
    "                        if t % 2 == 1:\n",
    "                            w_d1 = w_d1 + lr_p * DA_term_d1 * np.outer(post_efference_d1, state_onehot)\n",
    "                            w_d2 = w_d2 + lr_p * DA_term_d2 * np.outer(post_efference_d2, state_onehot)\n",
    "\n",
    "                            V[state] += lr_V * (r - V[state])\n",
    "\n",
    "                        if t % (2*record_every) == 0 and t % 2 == 0:\n",
    "                            d1_activity_results[rt, task_id, m, trial, t//(2*record_every), e] = post_efference_d1#post_efference_d1\n",
    "                            d2_activity_results[rt,task_id, m, trial, t//(2*record_every), e] = post_efference_d2#post_efference_d2\n",
    "\n",
    "                            action_label_results[rt,task_id, m, trial, t//(2*record_every), e] = action\n",
    "\n",
    "                            state_onehot = np.eye(num_states)\n",
    "                            a_d1 = np.matmul(w_d1, state_onehot)\n",
    "                            a_d2 = np.matmul(w_d2, state_onehot)\n",
    "                            a_d1 = np.clip(a_d1, 0, np.inf)\n",
    "                            a_d2 = np.clip(a_d2, 0, np.inf)\n",
    "\n",
    "                            #a_d2 = a_d2_mu+explore_noise*np.random.normal(0, 1, size=(num_neurons))\n",
    "                            Q_d1 = np.matmul(zeta_d1, a_d1)\n",
    "                            Q_d2 = np.matmul(zeta_d2, a_d2)\n",
    "                            Q = Q_d1 - Q_d2\n",
    "\n",
    "                            Q_results[rt,task_id, m, trial//2, t//record_every, e] = Q\n",
    "\n",
    "                        w_d1 = np.clip(w_d1, -10.0, 10.0)\n",
    "                        w_d2 = np.clip(w_d2, -10.0, 10.0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import SubplotSpec\n",
    "\n",
    "def create_subtitle(fig: plt.Figure, grid: SubplotSpec, title: str):\n",
    "    \"Sign sets of subplots with title\"\n",
    "    row = fig.add_subplot(grid)\n",
    "    # the '\\n' is important\n",
    "    row.set_title(f'{title}\\n', fontweight='semibold', fontsize=6)\n",
    "    # hide subplot\n",
    "    row.set_frame_on(False)\n",
    "    row.axis('off')\n",
    "\n",
    "cm = 1.0/2.54\n",
    "plt.rcParams[\"legend.markerscale\"] = 1.0\n",
    "plt.rcParams['legend.handlelength'] = 0.5\n",
    "colors = [[[1.0, 0.0, 1.0], [0.7, 0.0, 0.7], [0.4, 0.0, 0.4], [0.0, 0.0, 0.0]][::-1],\n",
    "          [[1.0, 0.0, 1.0], [0.7, 0.0, 0.7], [0.4, 0.0, 0.4], [0.0, 0.0, 0.0]][::-1]]\n",
    "          #[[0.0, 1.0, 0.0], [0.0, 0.7, 0.0], [0.0, 0.4, 0.0], [0.0, 0.0, 0.0]][::-1]]\n",
    "linestyles = [\"solid\", \"dotted\"]\n",
    "kernel = 10\n",
    "tasks_to_use = [0]\n",
    "rows = len(tasks_to_use)\n",
    "cols = 1\n",
    "\n",
    "skip = 10\n",
    "for rt in range(2):\n",
    "    print(reward_types[rt])\n",
    "\n",
    "    for offset in [0, 2]:\n",
    "        if offset == 0:\n",
    "            DA_type = \"linear\"\n",
    "        if offset == 2:\n",
    "            DA_type = \"rectified\"\n",
    "        print(DA_type)\n",
    "        fig, ax = plt.subplots(rows, cols, figsize=(4*cm, 4*cm*len(tasks_to_use)))\n",
    "        if rows == 1:\n",
    "            ax = np.array([ax])\n",
    "\n",
    "\n",
    "        for task_id in tasks_to_use:\n",
    "            t = tasks_to_use.index(task_id)\n",
    "            for m in range(offset, offset+2):\n",
    "                e = 0\n",
    "                for e in [0, 1, 2]:\n",
    "                    smoothed = [moving_average(r_results[rt, task_id, m, i, :, e], kernel) for i in range(num_trials)]\n",
    "                    ax[t].plot(range(timesteps-kernel+1)[skip::skip], np.mean(smoothed, 0)[:skip*(len(smoothed[0])//skip)].reshape(-1, skip).mean(1), linestyle=linestyles[m%2], color=colors[m%2][e], marker=\"o\", markersize=0)#, np.std(smoothed, 0)/np.sqrt(num_trials), color=colors[m%3])\n",
    "\n",
    "                #for jj in range(10):\n",
    "                #    ax[task_id, col].plot(range(timesteps-kernel+1), smoothed[jj], color=colors[m%3], alpha=0.5)#,#/np.sqrt(num_trials),\n",
    "\n",
    "\n",
    "                #print(m, np.mean(smoothed, 0)[-10:])\n",
    "            #ax[task_id, 0].legend(modes[:3])\n",
    "            if t == 0:\n",
    "                ax[t].legend([\"no efference\", \"efference\"], loc=\"lower center\", bbox_to_anchor=(0.7, -0.0))#, bbox_to_anchor=(0.6, 0.38))\n",
    "            for col in range(2):\n",
    "                ax[t].set_ylabel(\"Avg. reward\")\n",
    "                ax[t].set_xlabel(\"Steps\")\n",
    "                #if t < len(tasks_to_use)-1:\n",
    "                #    ax[t].set_xlabel(None)\n",
    "\n",
    "            #if task_id == 0:\n",
    "            #    ax[t].set_ylim([0, 1])\n",
    "            ax[t].set_xlim([0, 2000-kernel*skip])\n",
    "            ax[t].set_xticks([0, 2000-kernel*skip])\n",
    "            ax[t].set_xticklabels([0, 2000])\n",
    "\n",
    "\n",
    "            if tasks[task_id] == \"positive\":\n",
    "                ax[t].set_title(\"Reward for correct\")\n",
    "            if tasks[task_id] == \"negative\":\n",
    "                ax[t].set_title(\"Punish for incorrect\")\n",
    "            if tasks[task_id] == \"mix\":\n",
    "                ax[t].set_title(\"Mix (cue-dependent)\")\n",
    "\n",
    "        grid = plt.GridSpec(rows, cols)\n",
    "        #for r in range(rows):\n",
    "        #    create_subtitle(fig, grid[r, ::], np.array(tasks[r]).squeeze())\n",
    "        #create_subtitle(fig, grid[0, ::], 'Positive rewards')\n",
    "        #create_subtitle(fig, grid[1, ::], 'Negative rewards')\n",
    "        #create_subtitle(fig, grid[2, ::], 'Condition-contingent positive / negative rewards')\n",
    "        #create_subtitle(fig, grid[3, ::], 'Random rewards')\n",
    "        print(tasks_to_use)\n",
    "        fig.align_ylabels(ax)\n",
    "        fig.tight_layout()\n",
    "        if reward_types[rt] == \"r-Q\":\n",
    "            plt.savefig(\"Fig3_efference_vs_noefference_DA_\"+DA_type+\"_rt_\"+str(reward_types[rt])+\".pdf\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import SubplotSpec\n",
    "\n",
    "def create_subtitle(fig: plt.Figure, grid: SubplotSpec, title: str):\n",
    "    \"Sign sets of subplots with title\"\n",
    "    row = fig.add_subplot(grid)\n",
    "    # the '\\n' is important\n",
    "    row.set_title(f'{title}\\n', fontweight='semibold', fontsize=6)\n",
    "    # hide subplot\n",
    "    row.set_frame_on(False)\n",
    "    row.axis('off')\n",
    "\n",
    "cm = 1.0/2.54\n",
    "plt.rcParams[\"legend.markerscale\"] = 1.0\n",
    "plt.rcParams['legend.handlelength'] = 0.5\n",
    "colors = [[[0.2, 1.0, 0.0], [0.14, 0.7, 0.0], [0.08, 0.4, 0.0], [0.0, 0.0, 0.0]][::-1],\n",
    "          [[0.2, 1.0, 0.0], [0.14, 0.7, 0.0], [0.08, 0.4, 0.0], [0.0, 0.0, 0.0]][::-1]]\n",
    "colors = [[[1.0, 0.0, 1.0], [0.7, 0.0, 0.7], [0.4, 0.0, 0.4], [0.0, 0.0, 0.0]][::-1],\n",
    "          [[1.0, 0.0, 1.0], [0.7, 0.0, 0.7], [0.4, 0.0, 0.4], [0.0, 0.0, 0.0]][::-1]]\n",
    "          #[[0.0, 1.0, 0.0], [0.0, 0.7, 0.0], [0.0, 0.4, 0.0], [0.0, 0.0, 0.0]][::-1]]\n",
    "linestyles = [\"dotted\", \"solid\"]\n",
    "kernel = 10\n",
    "tasks_to_use = [0]\n",
    "rows = len(tasks_to_use)\n",
    "cols = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for offset in [0, 2]:\n",
    "    if offset == 0:\n",
    "        DA_type = \"linear\"\n",
    "    if offset == 2:\n",
    "        DA_type = \"rectified\"\n",
    "    print(DA_type)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(4*cm, 4*cm*len(tasks_to_use)))\n",
    "    if rows == 1:\n",
    "        ax = np.array([ax])\n",
    "\n",
    "\n",
    "    for task_id in tasks_to_use:\n",
    "        t = tasks_to_use.index(task_id)\n",
    "        m = offset+1\n",
    "        for rt in range(2):\n",
    "            e = 0\n",
    "            for e in [0, 1, 2]:\n",
    "                smoothed = [moving_average(r_results[rt, task_id, m, i, :, e], kernel) for i in range(num_trials)]\n",
    "                ax[t].plot(range(timesteps-kernel+1)[skip::skip], np.mean(smoothed, 0)[:skip*(len(smoothed[0])//skip)].reshape(-1, skip).mean(1), linestyle=linestyles[rt%2], color=colors[rt%2][e], marker=\"o\", markersize=0)#, np.std(smoothed, 0)/np.sqrt(num_trials), color=colors[m%3])\n",
    "\n",
    "            #for jj in range(10):\n",
    "            #    ax[task_id, col].plot(range(timesteps-kernel+1), smoothed[jj], color=colors[m%3], alpha=0.5)#,#/np.sqrt(num_trials),\n",
    "\n",
    "\n",
    "            #print(m, np.mean(smoothed, 0)[-10:])\n",
    "        #ax[task_id, 0].legend(modes[:3])\n",
    "        #if t == 0:\n",
    "        #    ax[t].legend([\"no\", \"efference\"], loc=\"lower center\", bbox_to_anchor=(0.7, -0.0))#, bbox_to_anchor=(0.6, 0.38))\n",
    "        for col in range(2):\n",
    "            ax[t].set_ylabel(\"Avg. reward\")\n",
    "            ax[t].set_xlabel(\"Steps\")\n",
    "            #if t < len(tasks_to_use)-1:\n",
    "            #    ax[t].set_xlabel(None)\n",
    "\n",
    "        #if task_id == 0:\n",
    "        #    ax[t].set_ylim([0, 1])\n",
    "        ax[t].set_xlim([0, 2000-kernel*skip])\n",
    "        ax[t].set_xticks([0, 2000-kernel*skip])\n",
    "        ax[t].set_xticklabels([0, 2000])\n",
    "\n",
    "\n",
    "        if tasks[task_id] == \"positive\":\n",
    "            ax[t].set_title(\"Reward for correct\")\n",
    "        if tasks[task_id] == \"negative\":\n",
    "            ax[t].set_title(\"Punish for incorrect\")\n",
    "        if tasks[task_id] == \"mix\":\n",
    "            ax[t].set_title(\"Mix (cue-dependent)\")\n",
    "\n",
    "    grid = plt.GridSpec(rows, cols)\n",
    "    #for r in range(rows):\n",
    "    #    create_subtitle(fig, grid[r, ::], np.array(tasks[r]).squeeze())\n",
    "    #create_subtitle(fig, grid[0, ::], 'Positive rewards')\n",
    "    #create_subtitle(fig, grid[1, ::], 'Negative rewards')\n",
    "    #create_subtitle(fig, grid[2, ::], 'Condition-contingent positive / negative rewards')\n",
    "    #create_subtitle(fig, grid[3, ::], 'Random rewards')\n",
    "    print(tasks_to_use)\n",
    "    fig.align_ylabels(ax)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"Fig5_Q_vs_V_DA_\"+DA_type+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a40",
   "language": "python",
   "name": "a40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
